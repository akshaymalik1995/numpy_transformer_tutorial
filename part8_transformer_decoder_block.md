# Part 8: Implementing the Transformer Decoder Block

Welcome to Part 8 of our "Transformer from Scratch with NumPy" series! Having built the `TransformerEncoder` in Part 7, we now turn our attention to its counterpart: the Transformer Decoder. In this part, we will construct a single `DecoderBlock`. The decoder has a slightly more complex structure than the encoder block because it needs to handle two types of attention mechanisms and generate an output sequence token by token.

## 8.1 Introduction to the Decoder Block

The decoder's role in the Transformer is to generate an output sequence (e.g., a translated sentence) based on the encoded representation of the input sequence. It does this auto-regressively, meaning it generates one token at a time, and the previously generated tokens are used as input to generate the next token.

A Transformer Decoder is typically composed of a stack of `N` identical `DecoderBlock`s. Each `DecoderBlock` has three main sub-layers:

1.  **Masked Multi-Head Self-Attention Mechanism:** This allows the decoder to attend to different positions in the output sequence generated so far. The "masked" part is crucial: it ensures that when predicting a token at position `i`, the self-attention mechanism can only attend to tokens at positions less than `i`. This prevents the model from "cheating" by looking at future tokens it hasn't predicted yet.
2.  **Encoder-Decoder Multi-Head Attention Mechanism:** This layer allows the decoder to attend to the output of the encoder (the contextualized representations of the input sequence). For each token being generated by the decoder, this mechanism helps decide which parts of the input sequence are most relevant.
3.  **Position-wise Feed-Forward Network (FFN):** Similar to the encoder, this is a fully connected feed-forward network applied independently to each position.

Like in the encoder block, residual connections are employed around each of these three sub-layers, followed by layer normalization.

The overall structure for one decoder block can be visualized as:

`Target Input -> Masked Multi-Head Attention -> Add & Norm -> Encoder-Decoder Attention -> Add & Norm -> Feed-Forward Network -> Add & Norm -> Output`

## 8.2 Components of the Decoder Block

We will reuse several components we've already built:
*   `MultiHeadAttention` (from Part 3): This will be used for both self-attention and encoder-decoder attention.
*   `PositionwiseFeedForward` (from Part 4).
*   `LayerNormalization` (from Part 6).
*   `dropout_layer` (from Part 6).

### 8.2.1 Masked Multi-Head Self-Attention

This is a standard multi-head self-attention mechanism, but with a crucial difference: a **look-ahead mask** (or subsequent mask) is applied. This mask prevents positions from attending to subsequent positions. For a target sequence of length `seq_len_tgt`, the mask will be an upper triangular matrix that, when applied, zeros out (or sets to a large negative number before softmax) the attention scores for future tokens.

### 8.2.2 Encoder-Decoder Attention

In this sub-layer:
*   **Queries (Q)** come from the output of the previous sub-layer in the decoder (the masked multi-head self-attention layer).
*   **Keys (K)** and **Values (V)** come from the output of the `TransformerEncoder` (i.e., the encoded representation of the source sequence).

This allows every position in the decoder to attend over all positions in the input sequence. The mask used here is typically a **padding mask** if the input sequence has padding, to prevent attention to `<pad>` tokens in the source.

## 8.3 NumPy Implementation of `DecoderBlock`

Let's define the `DecoderBlock` class. We'll bring in the necessary components again for a self-contained example. In a full project, you would import these.

```python
import numpy as np

# --- LayerNormalization class (from Part 6) ---
class LayerNormalization:
    def __init__(self, d_model, eps=1e-6):
        self.d_model = d_model
        self.eps = eps
        self.gamma = np.ones(d_model)
        self.beta = np.zeros(d_model)

    def forward(self, x):
        mean = np.mean(x, axis=-1, keepdims=True)
        variance = np.var(x, axis=-1, keepdims=True)
        x_normalized = (x - mean) / np.sqrt(variance + self.eps)
        output = self.gamma * x_normalized + self.beta
        return output

# --- dropout_layer function (from Part 6) ---
def dropout_layer(x, rate, training=True):
    if not training or rate == 0:
        return x
    mask = np.random.binomial(1, 1 - rate, size=x.shape) / (1 - rate)
    return x * mask

# --- Dummy MultiHeadAttention (Placeholder for Part 3's implementation) ---
# In a real implementation, this would be the actual MHA from Part 3.
class DummyMultiHeadAttention:
    def __init__(self, d_model, num_heads):
        self.d_model = d_model
        self.num_heads = num_heads
        # W_q, W_k, W_v, W_o would be initialized here.

    def forward(self, query, key, value, mask=None):
        # query: (batch_size, seq_len_q, d_model)
        # key:   (batch_size, seq_len_k, d_model)
        # value: (batch_size, seq_len_v, d_model) where seq_len_k == seq_len_v
        batch_size, seq_len_q, _ = query.shape
        # Simulate MHA output
        output = np.random.rand(batch_size, seq_len_q, self.d_model)
        # Dummy attention weights (batch_size, num_heads, seq_len_q, seq_len_k)
        seq_len_k = key.shape[1]
        attention_weights = np.random.rand(batch_size, self.num_heads, seq_len_q, seq_len_k)
        return output, attention_weights

# --- Dummy PositionwiseFeedForward (Placeholder for Part 4's implementation) ---
class DummyPositionwiseFeedForward:
    def __init__(self, d_model, d_ff):
        self.d_model = d_model
        self.d_ff = d_ff
        # Weights and biases for two linear layers would be here.

    def forward(self, x):
        # x: (batch_size, seq_len, d_model)
        # Simulate PFF output
        return np.random.rand(x.shape[0], x.shape[1], self.d_model)

# --- DecoderBlock class ---
class DecoderBlock:
    def __init__(self, self_attention_mha, encoder_decoder_attention_mha, 
                 positionwise_feed_forward, d_model, dropout_rate):
        """
        Initializes a Transformer Decoder Block.
        Args:
            self_attention_mha: MultiHeadAttention instance for masked self-attention.
            encoder_decoder_attention_mha: MultiHeadAttention instance for encoder-decoder attention.
            positionwise_feed_forward: PositionwiseFeedForward instance.
            d_model (int): The dimension of the model (embedding dimension).
            dropout_rate (float): The dropout rate.
        """
        self.self_attention_mha = self_attention_mha
        self.encoder_decoder_attention_mha = encoder_decoder_attention_mha
        self.positionwise_feed_forward = positionwise_feed_forward
        
        self.norm1 = LayerNormalization(d_model)
        self.norm2 = LayerNormalization(d_model)
        self.norm3 = LayerNormalization(d_model)
        
        self.dropout_rate = dropout_rate

    def forward(self, target_x, encoder_output, look_ahead_mask, padding_mask, training=True):
        """
        Forward pass for the Decoder Block.
        Args:
            target_x (np.ndarray): Input tensor for the decoder (e.g., target embeddings + pos_encoding),
                                   shape (batch_size, target_seq_len, d_model).
            encoder_output (np.ndarray): Output from the TransformerEncoder,
                                       shape (batch_size, source_seq_len, d_model).
            look_ahead_mask (np.ndarray): Mask for the self-attention sub-layer to prevent attending to future tokens.
                                          Shape (batch_size, 1, target_seq_len, target_seq_len).
            padding_mask (np.ndarray): Mask for the encoder-decoder attention sub-layer to hide padding in encoder_output.
                                       Shape (batch_size, 1, target_seq_len, source_seq_len).
            training (bool): Whether the model is in training mode (for dropout).
        Returns:
            np.ndarray: Output tensor, shape (batch_size, target_seq_len, d_model).
            np.ndarray: Attention weights from the self-attention mechanism.
            np.ndarray: Attention weights from the encoder-decoder attention mechanism.
        """
        # 1. Masked Multi-Head Self-Attention sub-layer
        # Q, K, V are all derived from target_x
        self_attn_output, self_attn_weights = self.self_attention_mha.forward(
            query=target_x, key=target_x, value=target_x, mask=look_ahead_mask
        )
        self_attn_output_dropout = dropout_layer(self_attn_output, self.dropout_rate, training)
        # Add residual connection and LayerNorm
        norm1_input = target_x + self_attn_output_dropout
        norm1_output = self.norm1.forward(norm1_input)

        # 2. Encoder-Decoder Multi-Head Attention sub-layer
        # Q comes from norm1_output (output of previous sub-layer)
        # K, V come from encoder_output
        enc_dec_attn_output, enc_dec_attn_weights = self.encoder_decoder_attention_mha.forward(
            query=norm1_output, key=encoder_output, value=encoder_output, mask=padding_mask
        )
        enc_dec_attn_output_dropout = dropout_layer(enc_dec_attn_output, self.dropout_rate, training)
        # Add residual connection and LayerNorm
        norm2_input = norm1_output + enc_dec_attn_output_dropout
        norm2_output = self.norm2.forward(norm2_input)

        # 3. Position-wise Feed-Forward sub-layer
        ffn_output = self.positionwise_feed_forward.forward(norm2_output)
        ffn_output_dropout = dropout_layer(ffn_output, self.dropout_rate, training)
        # Add residual connection and LayerNorm
        norm3_input = norm2_output + ffn_output_dropout
        output = self.norm3.forward(norm3_input)
        
        return output, self_attn_weights, enc_dec_attn_weights

```

## 8.4 Creating Masks for the Decoder

Two types of masks are typically needed for the decoder block:

1.  **Look-Ahead Mask (Subsequent Mask):** This is used in the self-attention mechanism of the decoder. It prevents any position from attending to subsequent positions. This is essential for auto-regressive generation, as the prediction for the current token should not depend on future tokens.
    A look-ahead mask for a sequence of length `L` is an `L x L` matrix where the upper triangle (elements `(i, j)` where `j > i`) is masked (e.g., set to 1 if 1 means mask, or a large negative number if applied before softmax).

2.  **Padding Mask:** This mask is used in the encoder-decoder attention mechanism. If the source sequence (input to the encoder) was padded to a fixed length, this mask ensures that the decoder does not attend to these `<pad>` tokens in the encoder's output.
    It also applies to the target sequence in the masked self-attention if the target sequence itself has padding.

Let's define a helper function to create a look-ahead mask.

```python
def create_look_ahead_mask(seq_len):
    """
    Creates a look-ahead mask for self-attention in the decoder.
    The mask is an upper triangular matrix of 1s (or True) where attention should be prevented.
    Args:
        seq_len (int): Length of the target sequence.
    Returns:
        np.ndarray: Look-ahead mask of shape (seq_len, seq_len).
                    Mask value 1 (or True) means do not attend.
    """
    mask = 1 - np.triu(np.ones((seq_len, seq_len)), k=1) # Lower triangle and diagonal are 1, upper is 0
    mask = np.triu(np.ones((seq_len, seq_len)), k=1) # Upper triangle is 1 (mask), rest is 0
    return mask # Shape: (seq_len, seq_len). 1 means mask, 0 means attend.
                # This needs to be broadcastable to (batch_size, num_heads, seq_len, seq_len)
                # Our scaled_dot_product_attention expects mask == 0 to attend, mask != 0 to prevent.
                # So, a mask value of 1 will correctly be interpreted as "prevent attention".

# Example of look_ahead_mask for seq_len = 3:
# [[0, 1, 1],
#  [0, 0, 1],
#  [0, 0, 0]]
# This means:
# - Token 0 can attend to token 0.
# - Token 1 can attend to tokens 0, 1.
# - Token 2 can attend to tokens 0, 1, 2.
```

A padding mask is simpler: it would typically be generated based on where the `<pad>` tokens are in the input sequences. For example, if `input_sequence` has `<pad>` tokens represented by 0, then `padding_mask = (input_sequence == 0)`. This boolean mask then needs to be reshaped and broadcast appropriately for the attention mechanism (e.g., `(batch_size, 1, 1, source_seq_len)` for encoder-decoder attention).

## 8.5 Simple Input/Output Example

Let's test our `DecoderBlock` with dummy data.

```python
# --- Example Usage ---
np.random.seed(4242)

# Parameters
batch_size = 2
target_seq_len = 12 # Length of the target sequence
source_seq_len = 10 # Length of the source sequence (from encoder)
d_model = 64       # Embedding dimension
num_heads = 4      # Number of attention heads
d_ff = 128         # Dimension of FFN hidden layer
dropout_rate = 0.1

# Instantiate dummy components
self_mha = DummyMultiHeadAttention(d_model, num_heads)
enc_dec_mha = DummyMultiHeadAttention(d_model, num_heads)
pff_instance = DummyPositionwiseFeedForward(d_model, d_ff)

# Instantiate the Decoder Block
decoder_block = DecoderBlock(self_mha, enc_dec_mha, pff_instance, d_model, dropout_rate)

# Create dummy input tensors
# Target input (e.g., shifted target embeddings + positional encoding)
# Shape: (batch_size, target_seq_len, d_model)
target_input_tensor = np.random.rand(batch_size, target_seq_len, d_model)

# Encoder output (from the TransformerEncoder)
# Shape: (batch_size, source_seq_len, d_model)
encoder_output_tensor = np.random.rand(batch_size, source_seq_len, d_model)

# Create masks
# 1. Look-ahead mask for self-attention
# Shape: (target_seq_len, target_seq_len)
look_ahead_mask_single = create_look_ahead_mask(target_seq_len)
# Reshape for batch and head broadcasting: (batch_size, 1, target_seq_len, target_seq_len)
# The MultiHeadAttention implementation should handle broadcasting from (target_seq_len, target_seq_len)
# or expect this full shape. Our dummy MHA doesn't use the mask values, but a real one would.
# For scaled_dot_product_attention, mask is (..., seq_len_q, seq_len_k)
# So, (batch_size, 1, target_seq_len, target_seq_len) is a common shape.
look_ahead_mask_batch = np.tile(look_ahead_mask_single[np.newaxis, np.newaxis, :, :], (batch_size, 1, 1, 1))

# 2. Padding mask for encoder-decoder attention (e.g., no padding in encoder output for this example)
# This mask prevents attention to padded tokens in the *source* sequence (encoder_output).
# Shape: (batch_size, 1, target_seq_len, source_seq_len)
# If no padding, mask is all zeros (attend to everything).
padding_mask_batch = np.zeros((batch_size, 1, target_seq_len, source_seq_len), dtype=int)

# Pass inputs through the decoder block
# Set training=False for consistent output (dropout is pass-through)
decoder_output, self_attn_w, enc_dec_attn_w = decoder_block.forward(
    target_input_tensor, 
    encoder_output_tensor, 
    look_ahead_mask_batch, 
    padding_mask_batch, 
    training=False
)

print("--- Decoder Block Example ---")
print("Target input tensor shape:", target_input_tensor.shape)
print("Encoder output tensor shape:", encoder_output_tensor.shape)
print("Look-ahead mask (batch) shape:", look_ahead_mask_batch.shape)
print("Padding mask (batch) shape:", padding_mask_batch.shape)
print("Decoder output tensor shape:", decoder_output.shape)
print("Self-attention weights shape:", self_attn_w.shape) # (batch_size, num_heads, target_seq_len, target_seq_len)
print("Encoder-decoder attention weights shape:", enc_dec_attn_w.shape) # (batch_size, num_heads, target_seq_len, source_seq_len)

print("
Sample of Decoder output (first item, first token, first 5 features):
", decoder_output[0, 0, :5])

# Verify output shape
if decoder_output.shape == (batch_size, target_seq_len, d_model):
    print("
Decoder output shape is correct!")
else:
    print("
Decoder output shape is INCORRECT!")

# Check a value in the look-ahead mask
print(f"
Look-ahead mask for target_seq_len={target_seq_len} (sample):")
print(look_ahead_mask_single)
# For scaled_dot_product_attention, if mask[b, h, q_idx, k_idx] != 0, then attention is prevented.
# So, look_ahead_mask_single[0,1] should be 1 (prevent Q0 from attending to K1).
# And look_ahead_mask_single[1,0] should be 0 (allow Q1 to attend to K0).

```

**Running the Example:**
When you run the combined Python code, you'll observe the shapes of all inputs, masks, and outputs. The `decoder_output` will have the shape `(batch_size, target_seq_len, d_model)`, which is `(2, 12, 64)` in this example. The attention weight shapes will also be printed, corresponding to the self-attention and encoder-decoder attention mechanisms.

## 8.6 Key Takeaways

*   A `DecoderBlock` has three main sub-layers: Masked Multi-Head Self-Attention, Encoder-Decoder Multi-Head Attention, and a Position-wise Feed-Forward Network.
*   Residual connections and Layer Normalization are applied around each sub-layer.
*   The **masked self-attention** uses a look-ahead mask to ensure auto-regressive behavior (preventing attention to future tokens).
*   The **encoder-decoder attention** allows the decoder to focus on relevant parts of the encoded input sequence, using the encoder's output as Keys and Values.
*   Appropriate masking (look-ahead and padding) is crucial for the correct functioning of the decoder.

## 8.7 What's Next?

With both the `EncoderBlock` (from Part 6/7) and the `DecoderBlock` now defined, we are ready to assemble the full Transformer model. In **Part 9: Building the Complete Transformer Architecture**, we will combine the `TransformerEncoder` (stack of `EncoderBlock`s), a `TransformerDecoder` (stack of `DecoderBlock`s), input/output embeddings, and a final linear layer to create the end-to-end Transformer model. Stay tuned!
